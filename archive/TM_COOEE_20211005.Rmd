---
title: "Analysis of Historical change in Irish English based on the HCIE - Part 1"
author: "Anonymous"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  bookdown::html_document2: default
bibliography: Bibliography.bib
link-citations: yes
---

# Introduction

This document shows an analysis of Irish English based on the Hamburg Corpus of Irish English (HCIE) [@pietsch2009hcie]. 

In a first step, the session is prepared by clearing the workspace, setting options, activating packages and loading relevant functions and defining the path to the data.

```{r ampause_01_01, echo=T, eval = T, message=FALSE, warning=FALSE}
# load packages
library(tidyverse)
library(tidytext)
library(tidyr)
library(tm)
library(tokenizers)
library(qdap)
# set options
options(stringsAsFactors = F)
options(scipen = 999)
# specify path to corpus
corpuspath <- "D:\\Uni\\Korpora\\Original\\HamburgCorpusofIrishEnglish"
# for pos-tagging objects in R
source("D:\\R/POStagObject.R") 
```

In a next step, we load and process the corpus data.


```{r load, warning=F, message=F}
# load corpus files
hciefiles = list.files(path = corpuspath, all.files = T, full.names = T, recursive = T, ignore.case = T, include.dirs = T)
# load and unlist corpus
hcie <- sapply(hciefiles, function(x) {
  x <- scan(x, what = "char", sep = "", quote = "\"", quiet = T, skipNul = T)
  x <- stringr::str_squish(x)
  x <- paste0(x, collapse = " ")
  x <- unlist(x)
} )
# inspect data
str(hcie)
```
Extract metadata (corpus, file, date, etc.)

```{r corpus, warning=F, message=F}
# corpus
Corpus <- sapply(hcie, function(x){
  x <- gsub(".*\\[\\^(.*?)\\^\\].*", "\\1", x)
})
# file
File <- sapply(hcie, function(x){
  x <- gsub(".*\\<T (.*?)\\>.*", "\\1", x)
})
# author
Author <- sapply(hcie, function(x){
  x <- gsub(".*\\<A (.*?)\\>.*", "\\1", x)
})
# date
Date <- sapply(hcie, function(x){
  x <- gsub(".*\\<O (.*?)\\>.*", "\\1", x)
})
# region
Region <- sapply(hcie, function(x){
  x <- gsub(".*\\<R (.*?)\\>", "\\1", x)
  x <- gsub(">.*", "", x)
})
# rawtext
Rawtext <- sapply(hcie, function(x){
  x <- gsub(".*\\[P  (.*)", "\\1", x)
})
# combine into table
hcied <- data.frame(Corpus, File, Author, Date, Region, Rawtext)
# inpsect
head(hcied)
```

Process and clean data

```{r clean, warning=F, message=F}
# clean files
hcied_clean <- hcied %>%
  dplyr::mutate(Text = Rawtext) %>%
  dplyr::filter(Date != "?> <R") %>%
  dplyr::mutate(Text = stringr::str_remove_all(Text, "\\[/{0,1}.*?\\]"),
                Text = stringr::str_remove_all(Text, "\\</{0,1}.*?\\>"),
                Text = stringr::str_remove_all(Text, "\\{|\\}"),
                Datecat = dplyr::case_when(Date < 1750 ~ "1675-1750",
                                           Date < 1800 ~ "1751-1800",
                                           Date < 1850 ~ "1801-1850",
                                           Date < 1900 ~ "1851-1900",  
                                           Date < 1950 ~ "1901-1950",
                                           TRUE ~ Date),
                Datecat = factor(Datecat),
                Id = 1:nrow(.)) %>%
  # Words
  dplyr::mutate(Words = stringr::str_replace_all(Text, "\\W", " "),
                Words = stringr::str_squish(Words),
                Words = stringr::str_count(Words, "\\w+"))
# inspect
head(hcied_clean)
```

```{r}
# define stopword regex
stopwords_regex = paste(tm::stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
# remove stopwords
hcied_clean <- hcied_clean %>%
  dplyr::mutate(Text = stringr::str_replace_all(tolower(Text), stopwords_regex, ""),
                Text = stringr::str_remove_all(Text, "[^[:alpha:] ]"))
# inspect
head(hcied_clean)
```



Check data

```{r}
table(hcied_clean$Datecat)
```



# Control corpus

We now generate a parallel table of data from letters not written by Irish emigrants

```{r control, warning=F, message=F}
controlpath1 <- "D:\\Uni\\Korpora\\Original\\Corpus of Late Modern English Texts\\plain_text\\1780-1850"
controlpath2 <- "D:\\Uni\\Korpora\\Original\\Corpus of Late Modern English Texts\\plain_text\\1850-1920"
# list files 
cfiles1 <- list.files(path = controlpath1, all.files = T, full.names = T, 
                      recursive = T, ignore.case = T, include.dirs = T, pattern = ".txt")
cfiles2 <- list.files(path = controlpath1, all.files = T, full.names = T, 
                      recursive = T, ignore.case = T, include.dirs = T, pattern = ".txt")
# combine files
cfiles <- c(cfiles1, cfiles2)
# load and unlist corpus
control <- sapply(cfiles, function(x) {
  x <- scan(x, what = "char", sep = "", quote = "\"", quiet = T, skipNul = T)
  x <- stringr::str_squish(x)
  x <- paste0(x, collapse = " ")
  x <- unlist(x)
} )
control <- control %>%
  as.data.frame() %>%
  dplyr::rename(Rawtext = colnames(.)[1])
# clean files
control_clean <- control %>%
  dplyr::mutate(Text = Rawtext) %>%
  dplyr::mutate(Text = stringr::str_remove_all(Text, "\\[/{0,1}.*?\\]"),
                Text = stringr::str_remove_all(Text, "\\</{0,1}.*?\\>"),
                Text = stringr::str_remove_all(Text, "\\{|\\}"),
                Text = stringr::str_remove_all(Text, "[^[:alpha:] ]"),
                Text = stringr::str_replace_all(tolower(Text), stopwords_regex, ""),
                Id = 1:nrow(.))
# inspect data
head(control_clean)
```

Create training set

```{r train, message=F, warning=F}
train_c <- control_clean %>%
  dplyr::mutate(Label = "NonIrish") %>%
  dplyr::select(Id, Text, Label)
train_hcie <- hcied_clean %>%
  dplyr::mutate(Label = "Irish") %>%
  dplyr::select(Id, Text, Label)
train <- rbind(train_hcie, train_c)
# inspect
head(train)
```


# Extract Keywords

Prepare for statistical extraction of keywords

```{r covtwit_01_11, echo=T, eval = T, message=FALSE, warning=FALSE}
# create table
keywordtb <- train %>%
  tidytext::unnest_tokens(Word, Text) %>%
  dplyr::group_by(Label) %>% 
  dplyr::count(Word, sort = TRUE) %>% 
  tidyr::spread(Label, n) %>% 
  replace(is.na(.), 0) %>%
  dplyr::mutate(Total = Irish+NonIrish) %>%
  dplyr::filter(Total > 10) %>%
  dplyr::mutate(TotalTarget = sum(Irish),
                TotalNonTarget = sum(NonIrish),
                NRows = length(Word)) %>%
  dplyr::rename(Target = Irish,
                NonTarget = NonIrish) %>%
  dplyr::select(-Total)
# inspect data
keywordtb
```

Perform statistics

```{r covtwit_01_12, echo=T, eval = T, message=FALSE, warning=FALSE}
source(here::here("scripts", "CoocStatzFisher.R"))
# extract keywords
keywords <- CoocStatzFisher(keywordtb)
sigkeywords <- keywords %>%
  dplyr::filter(CorrSignificance != "n.s.",
                Type == "Overuse")
# inspect data
sigkeywords$Word
```

Extract relative frequency of Irish letters

```{r covtwit_01_13, echo=T, eval = T, message=FALSE, warning=FALSE}
hcieterms <- sigkeywords$Word
hcie_regex <- paste0("\\b", hcieterms, "\\b|", collapse = "")
# extract frequencies
hciefreq <- hcied_clean %>%
  dplyr::select(Datecat, Text, Words) %>%
  dplyr::mutate(Frequency = stringr::str_count(Text, hcie_regex),
                RFrequency = Frequency/Words*100) %>%
  dplyr::group_by(Datecat) %>%
  dplyr::summarise(RelativeFrequency = mean(RFrequency)) %>%
  dplyr::mutate(NumDate = 1:length(Datecat),
                Datecat = factor(Datecat))
# inspect data
hciefreq
```


Plot relative frequency of covid-related tweets

```{r covtwit_01_14, echo=T, eval = T, message=FALSE, warning=FALSE}
ggplot(hciefreq, aes(x = NumDate, y = RelativeFrequency, label = round(RelativeFrequency, 2))) +
  geom_bar(stat = "identity") +
  scale_x_discrete(breaks = hciefreq$NumDate,
                   labels = hciefreq$Datecat,
                   limits = 1: max(hciefreq$NumDate)) +
  labs(x = "Date", y = "Relative Frequency of Keyterms\n(per 1,000 words)") +
  theme_set(theme_bw(base_size = 10)) +
  theme(legend.position="none",
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(angle = 90, size =7.5)) +
  geom_text(vjust=-1.6, color = "black") +
  coord_cartesian(ylim = c(0, 300))
```















Plot relative frequencies of covid terms

```{r covtwit_01_38, echo=T, eval = T, message=FALSE, warning=FALSE}
keywordselection <- c("home", "horse", "like", "famine", "country", "family")
# extract frequencies
kwfreqs <- hcied_clean %>%
  dplyr::select(Date, Text, Words, Datecat) %>%
  dplyr::group_by(Date) %>%
  dplyr::summarise(Text = paste(Text, collapse = " "),
                   Words = sum(Words),
                   Datecat = Datecat) %>%
  dplyr::ungroup() %>%
  unnest_tokens(Word, Text) %>%
  dplyr::filter(Word %in% keywordselection) %>%
  dplyr::group_by(Word, Date) %>%
  dplyr::summarise(NoWords = n(),
                   Percent = NoWords/unique(Words)*100,
                   Datecat = Datecat) %>%
  dplyr::ungroup() %>%
  dplyr::select(-NoWords) %>%
  dplyr::mutate(Date = factor(Date),
                NumDate = as.numeric(Date),
                Word = factor(Word))
# inspect data
head(kwfreqs)
```

Plot frequencies of selected keywords as line graph

```{r covtwit_01_39, echo=T, eval = T, message=FALSE, warning=FALSE}
p7d <- kwfreqs %>%
  dplyr::mutate(Date = factor(Date),
                NumDate = as.numeric(Date),
                Word = factor(Word))

ggplot(p7d, aes(x = NumDate, y = Percent)) +
  facet_wrap(vars(Word), ncol = 3, scales="free_y") +
  geom_line(color = "gray80", size = .5) +
  geom_smooth(se = F, span = .2, color = "gray20", size = .75) +
  scale_x_discrete(breaks = p7d$NumDate[seq(1, length(p7d$NumDate), by = 100)],
                   labels = p7d$Date[seq(1, length(p7d$Date), by = 100)],
                   limit = 1:length(table(p7d$Date))) +
  labs(x = "Date", y = "Percent (of words)") +
  theme_set(theme_bw(base_size = 12)) +
  theme(legend.position="none",
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(angle = 90, size =7))
```

With periods

```{r covtwit_01_40, echo=T, eval = T, message=FALSE, warning=FALSE}
p7d %>%
  dplyr::group_by(Datecat, Word) %>%
  dplyr::summarise(Percent = round(mean(Percent), 2)) %>%
  ggplot(aes(x = Datecat, y = Percent)) +
  facet_wrap(vars(Word), ncol = 3) +#, scales="free_y") +
  geom_bar(stat = "identity", size = .5) +
  labs(x = "", y = "Percent (of words)") +
  theme_set(theme_bw(base_size = 12)) +
  theme(legend.position="none",
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(angle = 90, size =7))
```


# Topic Modelling


Create corpus and DTM

```{r covtwit_02_03, echo=T, eval = T, message=FALSE, warning=FALSE}
# create corpus
hcied_clean <- hcied_clean %>%
  dplyr::mutate(doc_id = Id,
                text = Text)
corpus <- tm::Corpus(DataframeSource(hcied_clean))
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 10
DTM <- DocumentTermMatrix(corpus, 
                          control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
```

Due to vocabulary pruning, DTM may have empty rows (problematic!): remove empty docs from DTM and metadata

```{r covtwit_02_04, echo=T, eval = T, message=FALSE, warning=FALSE}
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
hcied_clean <- hcied_clean[sel_idx, ]
```

# Determine optimal number of topics

The determinination of the optimal number of topics follows @murzintcev2020idealtopics.

```{r covtwit_02_05, echo=T, eval = T, message=FALSE, warning=FALSE}
# load packages
library(topicmodels)
library(ldatuning)
# create sample data set
ldatuneset <- hcied_clean
ldatuneCorpus <- corpus <- Corpus(DataframeSource(ldatuneset))
ldatuneDTM <- DocumentTermMatrix(ldatuneCorpus,
                                 control = list(bounds = list(global = c(minimumFrequency, Inf))))
sel_idx <- slam::row_sums(ldatuneDTM) > 0
ldatuneDTM <- ldatuneDTM[sel_idx, ]
# create models with different number of topics
result <- FindTopicsNumber(
  ldatuneDTM,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)
```

Plot results (best number of topics: lowest CaoJuan2009, highest Griffith2004)

```{r covtwit_02_06, echo=T, eval = T, message=FALSE, warning=FALSE}
FindTopicsNumber_plot(result)
```

Results are volatile but there is a dip in minimizers and a peak with maximizers at 5. Thus, we select K = 5.

```{r covtwit_02_07, echo=T, eval = T, message=FALSE, warning=FALSE}
# number of topics
K <- 5
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
```

Extract LDA results

```{r covtwit_02_08, echo=T, eval = T, message=FALSE, warning=FALSE}
# have a look a some of the results (posterior distributions)
LDA_Result <- posterior(topicModel)
# topics are probability distribtions over the entire vocabulary
beta <- LDA_Result$terms   # get beta from results
# for every document we have a probaility distribution of its contained topics
theta <- as.data.frame(LDA_Result$topics)
# extract dominant topic for each tweet
hcied_clean$DominantTopic <- colnames(theta)[apply(theta,1,which.max)]
head(hcied_clean)
```

Write function for extracting distinctive terms per topic

```{r covtwit_02_09, echo=T, eval = T, message=FALSE, warning=FALSE}
ExtractAllTopicKeywords <- function(hcied_clean, Topic = 1){
  source(here::here("scripts", "CoocStatzFisher.R"))
  TopicTerms <- hcied_clean %>%
  unnest_tokens(Word, text) %>%
  dplyr::mutate(DominantTopic = ifelse(DominantTopic == Topic, "Target", "NonTarget")) %>%
  group_by(DominantTopic) %>% 
  count(Word, sort = TRUE) %>% 
  spread(DominantTopic, n) %>% 
  replace(is.na(.), 0) %>%
  mutate(Total = Target+NonTarget) %>%
  dplyr::filter(Total > 10) %>%
  dplyr::mutate(TotalTarget = sum(Target),
                TotalNonTarget = sum(NonTarget),
                NRows = length(Word)) %>%
  dplyr::select(-Total)
  # extract keywords
  keywords <- CoocStatzFisher(TopicTerms)
  sigkeywords <- keywords %>%
    dplyr::filter(CorrSignificance != "n.s.",
                  Type == "Overuse") %>%
    dplyr::arrange(-phi)
  return(sigkeywords)
}
```

Extract keyterms for topics

```{r covtwit_02_10, echo=T, eval = T, message=FALSE, warning=FALSE}
# apply function
AllSigKeywordsTopic01 <- ExtractAllTopicKeywords(hcied_clean, 1)
AllSigKeywordsTopic02 <- ExtractAllTopicKeywords(hcied_clean, 2)
AllSigKeywordsTopic03 <- ExtractAllTopicKeywords(hcied_clean, 3)
AllSigKeywordsTopic04 <- ExtractAllTopicKeywords(hcied_clean, 4)
AllSigKeywordsTopic05 <- ExtractAllTopicKeywords(hcied_clean, 5)
# determine number of key terms
NTerms <- 10
# combine tables
AllSigKeywordsTopic <- rbind(AllSigKeywordsTopic01[1:NTerms,], 
                             AllSigKeywordsTopic02[1:NTerms,], 
                             AllSigKeywordsTopic03[1:NTerms,], 
                             AllSigKeywordsTopic04[1:NTerms,], 
                             AllSigKeywordsTopic05[1:NTerms,])
# add Topic column
AllSigKeywordsTopic$Topic <- c(rep("Topic1", NTerms),
                               rep("Topic2", NTerms),
                               rep("Topic3", NTerms),
                               rep("Topic4", NTerms),
                               rep("Topic5", NTerms)) 
# clean table
AllSigKeywordsTopic <- AllSigKeywordsTopic %>%
  dplyr::select(-NonTarget, -TotalTarget, -TotalNonTarget, -RateTarget, -RateNonTarget, 
                -Type, -x2, -p, -Target)
# inspect data
head(AllSigKeywordsTopic)
```

```{r covtwit_02_11, echo=T, eval = T, message=FALSE, warning=FALSE}
KeywordsTopic <- AllSigKeywordsTopic %>%
  dplyr::mutate(Word = paste0(Word, " (", phi, "***)")) %>%
  dplyr::select(-CorrSignificance, -phi)
# inspect table
head(KeywordsTopic)
```

Write function for extracting the top six distinctive terms per topic

```{r covtwit_02_12, echo=T, eval = T, message=FALSE, warning=FALSE}
ExtractTopicKeywords <- function(hcied_clean, Topic = 1){
  source(here::here("scripts", "CoocStatzFisher.R"))
  TopicTerms <- hcied_clean %>%
  unnest_tokens(Word, text, token = "tweets") %>%
  dplyr::mutate(DominantTopic = ifelse(DominantTopic == Topic, "Target", "NonTarget")) %>%
  group_by(DominantTopic) %>% 
  count(Word, sort = TRUE) %>% 
  spread(DominantTopic, n) %>% 
  replace(is.na(.), 0) %>%
  mutate(Total = Target+NonTarget) %>%
  dplyr::filter(Total > 10) %>%
  dplyr::mutate(TotalTarget = sum(Target),
                TotalNonTarget = sum(NonTarget),
                NRows = length(Word)) %>%
  dplyr::select(-Total)
  # extract keywords
  keywords <- CoocStatzFisher(TopicTerms)
  sigkeywords <- keywords %>%
    dplyr::filter(CorrSignificance != "n.s.",
                  Type == "Overuse") %>%
    dplyr::arrange(-phi)
  return(paste(sigkeywords$Word[1:6], collapse = "|"))
}
```

Extract keyterms for topics

```{r covtwit_02_13, echo=T, eval = T, message=FALSE, warning=FALSE}
# apply function
SigKeywordsTopic01 <- ExtractTopicKeywords(hcied_clean, 1)
SigKeywordsTopic02 <- ExtractTopicKeywords(hcied_clean, 2)
SigKeywordsTopic03 <- ExtractTopicKeywords(hcied_clean, 3)
SigKeywordsTopic04 <- ExtractTopicKeywords(hcied_clean, 4)
SigKeywordsTopic05 <- ExtractTopicKeywords(hcied_clean, 5)
# inspect data
head(SigKeywordsTopic01)
```

Create topic names based on the most distinctive terms for each topic

```{r covtwit_02_14, echo=T, eval = T, message=FALSE, warning=FALSE}
topictermsls <- c(SigKeywordsTopic01, SigKeywordsTopic02, SigKeywordsTopic03,
                  SigKeywordsTopic04, SigKeywordsTopic05)
#topicNames <- paste("Topic", str_pad(1:5, 2, pad = "0"), topictermsls, sep = "_")
topicNames <- paste("Topic", 1:5, topictermsls, sep = "_")
topicNames <- str_replace_all(topicNames, "c_", "c")
topicNames
```

Manually created topic names

```{r covtwit_02_15, echo=T, eval = T, message=FALSE, warning=FALSE}
topicNames_manual <- c("Topic1_TRAVEL|LIVING", "Topic2_LOVELETTER", "Topic3_MILITARY",
                       "Topic4_FAMILY", "Topic5_INDIANS")
```



Add topics names to data

```{r covtwit_02_16, echo=T, eval = T, message=FALSE, warning=FALSE}
hcied_clean <- hcied_clean %>%
  dplyr::mutate(Topic = ifelse(DominantTopic == "1", topicNames_manual[1],
                        ifelse(DominantTopic == "2", topicNames_manual[2],
                        ifelse(DominantTopic == "3", topicNames_manual[3],
                        ifelse(DominantTopic == "4", topicNames_manual[4],
                               topicNames_manual[5]))))) %>%
  dplyr::rename(Period = Datecat)
# inspect data
head(hcied_clean)
```

Create Probability of Topics per Phase table

```{r covtwit_02_17, echo=T, eval = T, message=FALSE, warning=FALSE}
topicspd <- hcied_clean %>%
  dplyr::select(Period, Topic, Words) %>%
  dplyr::group_by(Period) %>%
  dplyr::mutate(SumWords = sum(Words)) %>%
  dplyr::group_by(Period, Topic) %>%
  dplyr::summarise(SumWords = unique(SumWords),
                   FrequencyTopic = n(),
                   Probability = FrequencyTopic/SumWords*100) %>%
  dplyr::ungroup()
# inspect data
head(topicspd)

```

Visualize results

```{r covtwit_02_18, echo=T, eval = T, message=FALSE, warning=FALSE}
require(pals)
ggplot(topicspd, aes(x=Period, y=Probability, fill=Topic)) +
  guides(fill=guide_legend(nrow = 3)) +
  geom_bar(stat = "identity") + 
  labs(y = "Percent (of words)", x = "Phase") + 
#  scale_x_continuous(breaks = seq(1, 7, 1), labels= seq(1, 7, 1)) +
  scale_fill_manual(labels = names(table(topicspd$Topic)),
                      breaks = names(table(topicspd$Topic)),
                      values = paste0("gray", seq(20, 80, 15)), 
                      name = "") +
  theme_set(theme_bw(base_size = 12)) +
  theme(legend.position="top",
        axis.text.x = element_text(angle = 0, hjust = 1, size = 10)) 
```

```{r covtwit_02_19, echo=T, eval = T, message=FALSE, warning=FALSE}
topicspd2 <- topicspd %>%
  dplyr::mutate(Topic = factor(Topic)) %>%
  dplyr::group_by(Period, Topic) %>%
  dplyr::summarise(Percent = mean(Probability))

ggplot(topicspd2, aes(x=Period, y=Percent, fill = Topic, linetype = Topic)) + 
  geom_bar(stat="identity", position="fill") +
#  geom_smooth(span = .75, se = F) + 
#  geom_line() +
  guides(color=guide_legend(nrow = 3)) +
  theme_set(theme_bw(base_size = 12)) +
  scale_fill_manual(labels = names(table(topicspd2$Topic)),
                      breaks = names(table(topicspd2$Topic)),
                      values = paste0("gray", seq(20, 80, 15)), 
                      name = "") +
  scale_linetype_manual(labels = names(table(topicspd2$Topic)),
                        breaks = names(table(topicspd2$Topic)),
                        values = seq(1, 5, 1), 
                        name = "") +
  labs(y = "Percent (of COVID-19 tweets)", x = "Phase") + 
#  scale_x_continuous(breaks = seq(1, 7, 1), 
#                     labels= seq(1, 7, 1), 
#                     limits = c(1, 7)) +
#  scale_y_continuous(limits = c(0, .5)) +
  theme(legend.position="top", 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(angle = 0, hjust = 1, size = 10))
```

```{r covtwit_02_20, echo=T, eval = T, message=FALSE, warning=FALSE}
ggplot(topicspd2, aes(x=Period, y=Percent, group = Topic, color = Topic, linetype = Topic)) + 
  geom_line(size = 1.25) + 
  guides(color=guide_legend(nrow = 5)) +
  scale_colour_manual(labels = names(table(topicspd2$Topic)),
                      breaks = names(table(topicspd2$Topic)),
                      values = paste0("gray", seq(20, 80, 15)), 
                      name = "") +
  scale_linetype_manual(labels = names(table(topicspd2$Topic)),
                        breaks = names(table(topicspd2$Topic)),
                        values = seq(1, 5, 1), 
                        name = "") +
  labs(y = "Percent (of COVID-19 tweets)", x = "Phase") + 
#  scale_x_continuous(breaks = seq(1, 7, 1), 
#                     labels= seq(1, 7, 1), 
#                     limits = c(1, 7)) +
  scale_y_continuous(limits = c(0, .3)) +
  theme(legend.position="top", 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(angle = 0, hjust = 1)) 

```

# Pronounts

```{r}
pp <- hcied_clean %>%
  dplyr::mutate(youse = stringr::str_count(Text, "\\byouse\\b|\\byous\\b"),
                ye = stringr::str_count(Text, "\\bye\\b")) %>%
  dplyr::group_by(Period) %>%
  dplyr::summarise(Words = sum(Words),
                   ye = sum(ye),
                   youse = sum(youse)) %>%
  dplyr::mutate(freq_youse = youse/Words*1000,
                freq_ye = ye/Words*1000,)
# inspect
head(pp)
```

```{r}
pp %>%
  tidyr::gather(Form, Frequency, freq_youse:freq_ye) %>%
  dplyr::mutate(Form = dplyr::case_when(Form == "freq_ye" ~ "ye",
                                        Form == "freq_youse" ~ "youse")) %>%
  ggplot(aes(x = Period, y = Frequency, group = Form, color = Form, linetype = Form)) +
  geom_line(size = 1.25) +
  theme_bw() +
  theme(legend.position = "top") +
    scale_colour_manual(labels = names(table(pp$Form)),
                      breaks = names(table(pp$Form)),
                      values = c("gray50", "gray80"), 
                      name = "") +
  scale_linetype_manual(labels = names(table(pp$Form)),
                        breaks = names(table(pp$Form)),
                        values = seq(1, 2, 1), 
                        name = "")
```

```{r}
pp2 <- hcied_clean %>%
  dplyr::mutate(youse = stringr::str_count(Text, "\\byouse\\b|\\byous\\b"),
                ye = stringr::str_count(Text, "\\bye\\b")) %>%
  dplyr::group_by(Period) %>%
  dplyr::mutate(freq_youse = youse/Words*1000,
                freq_ye = ye/Words*1000) %>%
  tidyr::gather(Form, Frequency, freq_youse:freq_ye) %>%
    dplyr::mutate(Form = dplyr::case_when(Form == "freq_ye" ~ "ye",
                                        Form == "freq_youse" ~ "youse"))

pp2 %>%
  ggplot(aes(x = Period, y = Frequency, group = Form, color = Form, linetype = Form, group = Form)) +
  geom_smooth() +
  theme_bw() +
  theme(legend.position = "top") +
    scale_colour_manual(labels = names(table(pp2$Form)),
                      breaks = names(table(pp2$Form)),
                      values = c("gray50", "gray80"), 
                      name = "") +
  scale_linetype_manual(labels = names(table(pp2$Form)),
                        breaks = names(table(pp2$Form)),
                        values = seq(1, 2, 1), 
                        name = "")
```


# Errors

```{r}
idx <- hcied_clean$Rawtext %>%
  stringr::str_count("\\[reg.*?\\/")
errors <- hcied_clean$Rawtext %>%
  stringr::str_extract_all("\\[reg.*?\\/") %>%
  unlist()
errorswona <- errors[!is.na(errors)]
```


```{r}
error <- hcied_clean %>%
  dplyr::select(Id, Corpus, File, Author, Date, Period) %>%
  dplyr::mutate(idx = idx) %>%
  dplyr::filter(idx != 0)
errortb <- error %>%
  dplyr::slice(rep(1:n(), idx)) %>%
  dplyr::mutate(errors = errorswona,
                correction = stringr::str_remove_all(errors, "\\[/.*")) %>%
  dplyr::mutate(correction = stringr::str_remove_all(correction, ".*\\]"),
                correction = stringr::str_squish(correction)) %>%
  dplyr::mutate(errors = stringr::str_remove_all(errors, "\\].*"),
                errors = stringr::str_remove_all(errors, ".*\\[reg"),
                errors = stringr::str_squish(errors)) %>%
  dplyr::select(-idx)
rownames(errortb) <- NULL
# inspect
head(errortb)
```


# Outro

```{r}
sessionInfo()
```


# References