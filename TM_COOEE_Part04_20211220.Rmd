---
title: "Analysis of the Corpus of Oz Early English - Part 4: Topic Modeling"
author: "Anonymous"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  bookdown::html_document2: default
bibliography: Bibliography.bib
link-citations: yes
---

# Introduction

This document shows an analysis of Corpus of Oz Early English  (COOEE) [@fritz2004cooee]. 

## Aims

* Find keywords

* Check for distribution of keyterms by date (mosaic plot) (part 2)

* Create network of keyterms (network analysis) (part 2)

* **Find topics (topic modeling)**

* Error analysis (misspelled words, words not in dictionary) (part 4)

# Set Up

In a first step, the session is prepared by installing packages.


```{r ampause_01_01, eval = F, message=FALSE, warning=FALSE}
# load packages
install.packages("tidyverse")
install.packages("tidytext")
install.packages("tidyr")
install.packages("tm")
install.packages("tokenizers")
install.packages("qdap")
install.packages("textstem")
install.packages("corpus")
```

Now, we activate the packages, set options, load relevant functions, and defining the path to the data.

```{r}
# set options
options(stringsAsFactors = F)
options(scipen = 999)
# load packages
library(tidyverse)
library(tidytext)
library(tidyr)
library(tm)
library(tokenizers)
library(qdap)
library(textstem)
library(corpus)
# for pos-tagging objects in R
source("D:\\R/POStagObject.R") 
```


In a next step, we load and process the data.

```{r}
# cooee
cooee_clean  <- base::readRDS(file = here::here("data", "cooee_clean.rda"))
# control
control_clean  <- base::readRDS(file = here::here("data", "control_clean.rda"))
```


# Topic Modelling


Create corpus and DTM

```{r covtwit_02_03, echo=T, eval = T, message=FALSE, warning=FALSE}
# create corpus
cooeed_min <- cooeed_clean %>%
  dplyr::mutate(doc_id = id,
                text = Text_stem)
corpus <- tm::Corpus(DataframeSource(cooeed_min))
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 1
DTM <- DocumentTermMatrix(corpus, 
                          control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
```

Due to vocabulary pruning, DTM may have empty rows (problematic!): remove empty docs from DTM and metadata

```{r covtwit_02_04, echo=T, eval = T, message=FALSE, warning=FALSE}
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
cooeed_min <- cooeed_min[sel_idx, ]
```

# Determine optimal number of topics

The determinination of the optimal number of topics follows @murzintcev2020idealtopics.

```{r covtwit_02_05, echo=T, eval = T, message=FALSE, warning=FALSE}
# load packages
library(topicmodels)
library(ldatuning)
# create sample data set
ldatuneset <- cooeed_min
ldatuneCorpus <- corpus <- Corpus(DataframeSource(ldatuneset))
ldatuneDTM <- DocumentTermMatrix(ldatuneCorpus,
                                 control = list(bounds = list(global = c(minimumFrequency, Inf))))
sel_idx <- slam::row_sums(ldatuneDTM) > 0
ldatuneDTM <- ldatuneDTM[sel_idx, ]
# create models with different number of topics
result <- FindTopicsNumber(
  ldatuneDTM,
  topics = seq(from = 2, to = 10, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014", "Griffiths2004"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)
```

Plot results (best number of topics: lowest CaoJuan2009, highest Griffith2004)

```{r covtwit_02_06, echo=T, eval = T, message=FALSE, warning=FALSE}
FindTopicsNumber_plot(result)
```

Results are volatile but there is a dip in minimizers  at 6. Thus, we select K = 6.

```{r covtwit_02_07, echo=T, eval = T, message=FALSE, warning=FALSE}
# number of topics
K <- 6
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
```

Extract LDA results

```{r covtwit_02_08, echo=T, eval = T, message=FALSE, warning=FALSE}
# have a look a some of the results (posterior distributions)
LDA_Result <- posterior(topicModel)
# topics are probability distributions over the entire vocabulary
beta <- LDA_Result$terms   # get beta from results
# for every document we have a probability distribution of its contained topics
theta <- as.data.frame(LDA_Result$topics)
# extract dominant topic for each tweet
cooeed_min$DominantTopic <- colnames(theta)[apply(theta,1,which.max)]
head(cooeed_min)
```

Write function for extracting distinctive terms per topic

```{r covtwit_02_09, echo=T, eval = T, message=FALSE, warning=FALSE}
ExtractAllTopicKeywords <- function(cooeed_min, Topic){
  source(here::here("scripts", "CoocStatzFisher.R"))
  TopicTerms <- cooeed_min %>%
  unnest_tokens(Word, text) %>%
  dplyr::mutate(DominantTopic = ifelse(DominantTopic == Topic, "Target", "NonTarget")) %>%
  group_by(DominantTopic) %>% 
  count(Word, sort = TRUE) %>% 
  spread(DominantTopic, n) %>% 
  replace(is.na(.), 0) %>%
  mutate(Total = Target+NonTarget) %>%
#  dplyr::filter(Total > 10) %>%
  dplyr::mutate(TotalTarget = sum(Target),
                TotalNonTarget = sum(NonTarget),
                NRows = length(Word)) %>%
  dplyr::select(-Total)
  # extract keywords
  keywords <- CoocStatzFisher(TopicTerms)
  sigkeywords <- keywords %>%
    dplyr::filter(CorrSignificance != "n.s.",
                  Type == "Overuse") %>%
    dplyr::arrange(-phi)
  return(sigkeywords)
}
```

Extract keyterms for topics

```{r covtwit_02_10, echo=T, eval = T, message=FALSE, warning=FALSE}
# apply function
AllSigKeywordsTopic01 <- ExtractAllTopicKeywords(cooeed_min, 1)
AllSigKeywordsTopic02 <- ExtractAllTopicKeywords(cooeed_min, 2)
AllSigKeywordsTopic03 <- ExtractAllTopicKeywords(cooeed_min, 3)
# determine number of key terms
NTerms <- 10
# combine tables
AllSigKeywordsTopic <- rbind(AllSigKeywordsTopic01[1:NTerms,], 
                             AllSigKeywordsTopic02[1:NTerms,], 
                             AllSigKeywordsTopic03[1:NTerms,],)
# add Topic column
AllSigKeywordsTopic$Topic <- c(rep("Topic1", NTerms),
                               rep("Topic2", NTerms),
                               rep("Topic3", NTerms)) 
# clean table
AllSigKeywordsTopic <- AllSigKeywordsTopic %>%
  dplyr::select(-NonTarget, -TotalTarget, -TotalNonTarget, -RateTarget, -RateNonTarget, 
                -Type, -x2, -p, -Target)
# inspect data
head(AllSigKeywordsTopic)
```

```{r covtwit_02_11, echo=T, eval = T, message=FALSE, warning=FALSE}
KeywordsTopic <- AllSigKeywordsTopic %>%
  dplyr::mutate(Word = paste0(Word, " (", phi, "***)")) %>%
  dplyr::select(-CorrSignificance, -phi)
# inspect table
head(KeywordsTopic)
```

Write function for extracting the top six distinctive terms per topic

```{r covtwit_02_12, echo=T, eval = T, message=FALSE, warning=FALSE}
ExtractTopicKeywords <- function(cooeed_clean, Topic = 1){
  source(here::here("scripts", "CoocStatzFisher.R"))
  TopicTerms <- cooeed_clean %>%
  unnest_tokens(Word, text, token = "tweets") %>%
  dplyr::mutate(DominantTopic = ifelse(DominantTopic == Topic, "Target", "NonTarget")) %>%
  group_by(DominantTopic) %>% 
  count(Word, sort = TRUE) %>% 
  spread(DominantTopic, n) %>% 
  replace(is.na(.), 0) %>%
  mutate(Total = Target+NonTarget) %>%
  dplyr::filter(Total > 10) %>%
  dplyr::mutate(TotalTarget = sum(Target),
                TotalNonTarget = sum(NonTarget),
                NRows = length(Word)) %>%
  dplyr::select(-Total)
  # extract keywords
  keywords <- CoocStatzFisher(TopicTerms)
  sigkeywords <- keywords %>%
    dplyr::filter(CorrSignificance != "n.s.",
                  Type == "Overuse") %>%
    dplyr::arrange(-phi)
  return(paste(sigkeywords$Word[1:6], collapse = "|"))
}
```

Extract keyterms for topics

```{r covtwit_02_13, echo=T, eval = T, message=FALSE, warning=FALSE}
# apply function
SigKeywordsTopic01 <- ExtractTopicKeywords(cooeed_clean, 1)
SigKeywordsTopic02 <- ExtractTopicKeywords(cooeed_clean, 2)
SigKeywordsTopic03 <- ExtractTopicKeywords(cooeed_clean, 3)
SigKeywordsTopic04 <- ExtractTopicKeywords(cooeed_clean, 4)
SigKeywordsTopic05 <- ExtractTopicKeywords(cooeed_clean, 5)
# inspect data
head(SigKeywordsTopic01)
```

Create topic names based on the most distinctive terms for each topic

```{r covtwit_02_14, echo=T, eval = T, message=FALSE, warning=FALSE}
topictermsls <- c(SigKeywordsTopic01, SigKeywordsTopic02, SigKeywordsTopic03,
                  SigKeywordsTopic04, SigKeywordsTopic05)
#topicNames <- paste("Topic", str_pad(1:5, 2, pad = "0"), topictermsls, sep = "_")
topicNames <- paste("Topic", 1:5, topictermsls, sep = "_")
topicNames <- str_replace_all(topicNames, "c_", "c")
topicNames
```

Manually created topic names

```{r covtwit_02_15, echo=T, eval = T, message=FALSE, warning=FALSE}
topicNames_manual <- c("Topic1_TRAVEL|LIVING", "Topic2_LOVELETTER", "Topic3_MILITARY",
                       "Topic4_FAMILY", "Topic5_INDIANS")
```



Add topics names to data

```{r covtwit_02_16, echo=T, eval = T, message=FALSE, warning=FALSE}
cooeed_clean <- cooeed_clean %>%
  dplyr::mutate(Topic = ifelse(DominantTopic == "1", topicNames_manual[1],
                        ifelse(DominantTopic == "2", topicNames_manual[2],
                        ifelse(DominantTopic == "3", topicNames_manual[3],
                        ifelse(DominantTopic == "4", topicNames_manual[4],
                               topicNames_manual[5]))))) %>%
  dplyr::rename(Period = Datecat)
# inspect data
head(cooeed_clean)
```

Create Probability of Topics per Phase table

```{r covtwit_02_17, echo=T, eval = T, message=FALSE, warning=FALSE}
topicspd <- cooeed_clean %>%
  dplyr::select(Period, Topic, Words) %>%
  dplyr::group_by(Period) %>%
  dplyr::mutate(SumWords = sum(Words)) %>%
  dplyr::group_by(Period, Topic) %>%
  dplyr::summarise(SumWords = unique(SumWords),
                   FrequencyTopic = n(),
                   Probability = FrequencyTopic/SumWords*100) %>%
  dplyr::ungroup()
# inspect data
head(topicspd)

```

Visualize results

```{r covtwit_02_18, echo=T, eval = T, message=FALSE, warning=FALSE}
require(pals)
ggplot(topicspd, aes(x=Period, y=Probability, fill=Topic)) +
  guides(fill=guide_legend(nrow = 3)) +
  geom_bar(stat = "identity") + 
  labs(y = "Percent (of words)", x = "Phase") + 
#  scale_x_continuous(breaks = seq(1, 7, 1), labels= seq(1, 7, 1)) +
  scale_fill_manual(labels = names(table(topicspd$Topic)),
                      breaks = names(table(topicspd$Topic)),
                      values = paste0("gray", seq(20, 80, 15)), 
                      name = "") +
  theme_set(theme_bw(base_size = 12)) +
  theme(legend.position="top",
        axis.text.x = element_text(angle = 0, hjust = 1, size = 10)) 
```

```{r covtwit_02_19, echo=T, eval = T, message=FALSE, warning=FALSE}
topicspd2 <- topicspd %>%
  dplyr::mutate(Topic = factor(Topic)) %>%
  dplyr::group_by(Period, Topic) %>%
  dplyr::summarise(Percent = mean(Probability))

ggplot(topicspd2, aes(x=Period, y=Percent, fill = Topic, linetype = Topic)) + 
  geom_bar(stat="identity", position="fill") +
#  geom_smooth(span = .75, se = F) + 
#  geom_line() +
  guides(color=guide_legend(nrow = 3)) +
  theme_set(theme_bw(base_size = 12)) +
  scale_fill_manual(labels = names(table(topicspd2$Topic)),
                      breaks = names(table(topicspd2$Topic)),
                      values = paste0("gray", seq(20, 80, 15)), 
                      name = "") +
  scale_linetype_manual(labels = names(table(topicspd2$Topic)),
                        breaks = names(table(topicspd2$Topic)),
                        values = seq(1, 5, 1), 
                        name = "") +
  labs(y = "Percent (of COVID-19 tweets)", x = "Phase") + 
#  scale_x_continuous(breaks = seq(1, 7, 1), 
#                     labels= seq(1, 7, 1), 
#                     limits = c(1, 7)) +
#  scale_y_continuous(limits = c(0, .5)) +
  theme(legend.position="top", 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(angle = 0, hjust = 1, size = 10))
```

```{r covtwit_02_20, echo=T, eval = T, message=FALSE, warning=FALSE}
ggplot(topicspd2, aes(x=Period, y=Percent, group = Topic, color = Topic, linetype = Topic)) + 
  geom_line(size = 1.25) + 
  guides(color=guide_legend(nrow = 5)) +
  scale_colour_manual(labels = names(table(topicspd2$Topic)),
                      breaks = names(table(topicspd2$Topic)),
                      values = paste0("gray", seq(20, 80, 15)), 
                      name = "") +
  scale_linetype_manual(labels = names(table(topicspd2$Topic)),
                        breaks = names(table(topicspd2$Topic)),
                        values = seq(1, 5, 1), 
                        name = "") +
  labs(y = "Percent (of COVID-19 tweets)", x = "Phase") + 
#  scale_x_continuous(breaks = seq(1, 7, 1), 
#                     labels= seq(1, 7, 1), 
#                     limits = c(1, 7)) +
  scale_y_continuous(limits = c(0, .3)) +
  theme(legend.position="top", 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        axis.text.x = element_text(angle = 0, hjust = 1)) 

```



# Outro

```{r}
sessionInfo()
```


# References

